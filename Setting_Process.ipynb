{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern3 in /anaconda3/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: simplejson in /anaconda3/lib/python3.7/site-packages (from pattern3) (3.17.2)\n",
      "Requirement already satisfied: pdfminer.six in /anaconda3/lib/python3.7/site-packages (from pattern3) (20200726)\n",
      "Requirement already satisfied: docx in /anaconda3/lib/python3.7/site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: cherrypy in /anaconda3/lib/python3.7/site-packages (from pattern3) (18.6.0)\n",
      "Requirement already satisfied: feedparser in /anaconda3/lib/python3.7/site-packages (from pattern3) (5.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /anaconda3/lib/python3.7/site-packages (from pattern3) (4.6.3)\n",
      "Requirement already satisfied: pdfminer3k in /anaconda3/lib/python3.7/site-packages (from pattern3) (1.3.4)\n",
      "Requirement already satisfied: sortedcontainers in /anaconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (2.1.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /anaconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (3.0.4)\n",
      "Requirement already satisfied: cryptography in /anaconda3/lib/python3.7/site-packages (from pdfminer.six->pattern3) (2.4.2)\n",
      "Requirement already satisfied: Pillow>=2.0 in /anaconda3/lib/python3.7/site-packages (from docx->pattern3) (5.3.0)\n",
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.7/site-packages (from docx->pattern3) (4.2.5)\n",
      "Requirement already satisfied: more-itertools in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (4.3.0)\n",
      "Requirement already satisfied: zc.lockfile in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (8.4.5)\n",
      "Requirement already satisfied: jaraco.collections in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: portend>=2.1.1 in /anaconda3/lib/python3.7/site-packages (from cherrypy->pattern3) (2.6)\n",
      "Requirement already satisfied: ply in /anaconda3/lib/python3.7/site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.7 in /anaconda3/lib/python3.7/site-packages (from cryptography->pdfminer.six->pattern3) (1.11.5)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /anaconda3/lib/python3.7/site-packages (from cryptography->pdfminer.six->pattern3) (0.24.0)\n",
      "Requirement already satisfied: idna>=2.1 in /anaconda3/lib/python3.7/site-packages (from cryptography->pdfminer.six->pattern3) (2.8)\n",
      "Requirement already satisfied: six>=1.4.1 in /anaconda3/lib/python3.7/site-packages (from cryptography->pdfminer.six->pattern3) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from zc.lockfile->cherrypy->pattern3) (40.6.3)\n",
      "Requirement already satisfied: jaraco.functools in /anaconda3/lib/python3.7/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.0.1)\n",
      "Requirement already satisfied: jaraco.classes in /anaconda3/lib/python3.7/site-packages (from jaraco.collections->cherrypy->pattern3) (3.1.0)\n",
      "Requirement already satisfied: jaraco.text in /anaconda3/lib/python3.7/site-packages (from jaraco.collections->cherrypy->pattern3) (3.2.0)\n",
      "Requirement already satisfied: tempora>=1.8 in /anaconda3/lib/python3.7/site-packages (from portend>=2.1.1->cherrypy->pattern3) (4.0.0)\n",
      "Requirement already satisfied: pycparser in /anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.7->cryptography->pdfminer.six->pattern3) (2.19)\n",
      "Requirement already satisfied: pytz in /anaconda3/lib/python3.7/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2018.7)\n",
      "Requirement already satisfied: pyLDAvis in /anaconda3/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.23.4)\n",
      "Requirement already satisfied: funcy in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.14)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.17.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.15.4)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.16.0)\n",
      "Requirement already satisfied: numexpr in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.6.8)\n",
      "Requirement already satisfied: pytest in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (4.0.2)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2018.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (40.6.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (18.2.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.2.1)\n",
      "Requirement already satisfied: pluggy>=0.7 in /anaconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /anaconda3/lib/python3.7/site-packages (from pluggy>=0.7->pytest->pyLDAvis) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pluggy>=0.7->pytest->pyLDAvis) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import html.parser\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import unicodedata\n",
    "import string\n",
    "!pip install pattern3 \n",
    "import pattern3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from pprint import pprint\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting translate\n",
      "  Using cached https://files.pythonhosted.org/packages/85/b2/2ea329a07bbc0c7227eef84ca89ffd6895e7ec237d6c0b26574d56103e53/translate-3.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tox in /anaconda3/lib/python3.7/site-packages (from translate) (3.20.0)\n",
      "Requirement already satisfied: click in /anaconda3/lib/python3.7/site-packages (from translate) (7.0)\n",
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.7/site-packages (from translate) (4.2.5)\n",
      "Collecting pre-commit (from translate)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/88/966da62ec93566c221bf8bf0c1b7f04e70f002960a98da05d3fd8bc3b2e7/pre_commit-2.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from translate) (2.21.0)\n",
      "Requirement already satisfied: pluggy>=0.12.0 in /anaconda3/lib/python3.7/site-packages (from tox->translate) (0.13.1)\n",
      "Requirement already satisfied: toml>=0.9.4 in /anaconda3/lib/python3.7/site-packages (from tox->translate) (0.10.1)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /anaconda3/lib/python3.7/site-packages (from tox->translate) (3.0.10)\n",
      "Requirement already satisfied: packaging>=14 in /anaconda3/lib/python3.7/site-packages (from tox->translate) (18.0)\n",
      "Requirement already satisfied: virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0 in /anaconda3/lib/python3.7/site-packages (from tox->translate) (20.0.31)\n",
      "Requirement already satisfied: importlib-metadata<2,>=0.12; python_version < \"3.8\" in /anaconda3/lib/python3.7/site-packages (from tox->translate) (1.7.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /anaconda3/lib/python3.7/site-packages (from tox->translate) (1.15.0)\n",
      "Requirement already satisfied: py>=1.4.17 in /anaconda3/lib/python3.7/site-packages (from tox->translate) (1.7.0)\n",
      "Collecting nodeenv>=0.11.1 (from pre-commit->translate)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/d0/efdf54539948315cc76e5a66b709212963101d002822c3b54369dbf9b5e0/nodeenv-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: identify>=1.0.0 in /anaconda3/lib/python3.7/site-packages (from pre-commit->translate) (1.5.0)\n",
      "Collecting pyyaml>=5.1 (from pre-commit->translate)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from pre-commit->translate) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->translate) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->translate) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->translate) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->translate) (1.24.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from packaging>=14->tox->translate) (2.3.0)\n",
      "Requirement already satisfied: appdirs<2,>=1.4.3 in /anaconda3/lib/python3.7/site-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox->translate) (1.4.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /anaconda3/lib/python3.7/site-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox->translate) (0.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda3/lib/python3.7/site-packages (from importlib-metadata<2,>=0.12; python_version < \"3.8\"->tox->translate) (3.1.0)\n",
      "Installing collected packages: nodeenv, pyyaml, pre-commit, translate\n",
      "  Found existing installation: PyYAML 3.13\n",
      "\u001b[31mCannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install translate\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "def pos_tag_text(text_tokens):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None  \n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text \n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_accented_characters(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
    "    return text\n",
    "def normalize_corpus(corpus, only_text_chars=True):\n",
    "    normalized_corpus = []  \n",
    "    for index, text in enumerate(corpus):\n",
    "        text = normalize_accented_characters(text)\n",
    "        text = expand_contractions(text, contraction_mapping)\n",
    "        text = tokenize_text(text)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text)\n",
    "        #text = tokenize_text(text)\n",
    "        normalized_corpus.append(text)    \n",
    "    return normalized_corpus\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
